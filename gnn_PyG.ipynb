{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GNN Book Recommender PyG implementation (attempt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import HeteroData\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import HeteroConv, SAGEConv, Linear\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch_geometric.transforms as T\n",
    "from torch.optim import Adam\n",
    "from torch.nn import BCELoss\n",
    "from torch_geometric.loader import NeighborLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "ratings = pd.read_csv(\"Data/Ratings.csv\")   # contains user-book ratings\n",
    "books = pd.read_csv(\"Data/Books.csv\", dtype={3: str})   # contains book attributes\n",
    "users = pd.read_csv(\"Data/Users.csv\")   # contains user profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will only use users and books present in the ratings dataset \n",
    "lessen_user_ids = {userid: idx for idx, userid in enumerate(ratings['User-ID'].unique())} #renumber IDs to reduce inactive users\n",
    "ratings['New-User-ID'] = ratings['User-ID'].map(lessen_user_ids)\n",
    "user_ids = list(ratings['New-User-ID'].unique())\n",
    "num_users = len(set(user_ids))\n",
    "\n",
    "\n",
    "# Map book identifiers (ISBN) to a unique integer identifier for datatype compatibility of dgl\n",
    "isbn_to_id = {isbn: idx for idx, isbn in enumerate(ratings['ISBN'].unique())}\n",
    "ratings['Book-ID'] = ratings['ISBN'].map(isbn_to_id)\n",
    "book_ids = list(ratings['Book-ID'].unique())\n",
    "num_books = len(set(book_ids))\n",
    "print(f'There are {len(user_ids)} unique users, and {len(book_ids)} unique books in the ratings dataset.')\n",
    " \n",
    "# Remove books not included in the ratings dataset\n",
    "books['Book-ID'] = books['ISBN'].map(isbn_to_id)\n",
    "books_clean = books[books['Book-ID'].isin(book_ids)]\n",
    "books_clean_ids = books_clean['Book-ID'].unique()\n",
    "percent_books_missing = round((num_books-len(books_clean_ids))/num_books*100, 0)\n",
    "print(f'There are around {percent_books_missing}% of books in the graph missing in the books data')\n",
    "\n",
    "# Remove users that are not included in the ratings dataset\n",
    "users['New-User-ID'] = users['User-ID'].map(lessen_user_ids)\n",
    "users_clean = users[users['New-User-ID'].isin(user_ids)]\n",
    "print(f\"There are: {len(users_clean['New-User-ID'])}, who have rated at least one book\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge books and ratings dataset\n",
    "ratings_with_book_titles = ratings.merge(books,on='ISBN')\n",
    "ratings_with_book_titles.drop(columns=[\"ISBN\",\"Image-URL-S\",\"Image-URL-M\"],axis=1,inplace=True)\n",
    "# Drop Age because tooo many missing values\n",
    "complete_df = ratings_with_book_titles.merge(users.drop(\"Age\", axis=1), on=\"User-ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge user location data and ratings\n",
    "complete_df['Location'] = complete_df['Location'].str.split(',').str[-1].str.strip()\n",
    "print(complete_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove books with no rating informaiton\n",
    "df = complete_df.loc[complete_df['Book-Rating'] != 0]\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove anomaly due to data entry errors\n",
    "df = df[df['Year-Of-Publication'] != 'DK Publishing Inc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'There are {len(df['User-ID'].unique())} users in the dataset.\\n')\n",
    "print(f'There are {len(df['Book-Title'].unique())} booksin the dataset.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start with graph structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = HeteroData() #initialize heterogenous graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique users\n",
    "unique_users = df[['User-ID', 'Location']].drop_duplicates()\n",
    "\n",
    "# Take care of the User-IDs\n",
    "user_id_encoder = LabelEncoder()\n",
    "df['encoded-user-ID'] = user_id_encoder.fit_transform(df['User-ID'].astype(str))    # Do so otherwise it is going to create a problem with tensor indexing later\n",
    "unique_users['User-ID'] = df['encoded-user-ID']\n",
    "\n",
    "# Encode the Location feature\n",
    "location_encoder = LabelEncoder()\n",
    "unique_users['Location'] = location_encoder.fit_transform(unique_users['Location'])\n",
    "\n",
    "# Get user tensor\n",
    "user_features_tensor = torch.tensor(unique_users[['User-ID', 'Location']].values, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['users'].x = user_features_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['users'].x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Book nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a composite key that uniquely identifies each book\n",
    "df['Book-Key'] = df['Book-Title'] + '|' + df['Book-Author'] + '|' + df['Publisher'] + '|' + df['Year-Of-Publication'].astype(str)\n",
    "\n",
    "# Now, get unique books using the new composite key\n",
    "unique_books = df[['Book-Key', 'Book-Title', 'Book-Author', 'Publisher', 'Year-Of-Publication']].drop_duplicates()\n",
    "\n",
    "# Encode categorical features\n",
    "book_key_encoder = LabelEncoder()\n",
    "unique_books['Book-Key'] = book_key_encoder.fit_transform(unique_books['Book-Key'])\n",
    "\n",
    "title_encoder = LabelEncoder()\n",
    "author_encoder = LabelEncoder()\n",
    "publisher_encoder = LabelEncoder()\n",
    "\n",
    "unique_books['Book-Title'] = title_encoder.fit_transform(unique_books['Book-Title'])\n",
    "unique_books['Book-Author'] = author_encoder.fit_transform(unique_books['Book-Author'])\n",
    "unique_books['Publisher'] = publisher_encoder.fit_transform(unique_books['Publisher'])\n",
    "\n",
    "# Normalize year of publication\n",
    "unique_books['Year-Of-Publication'] = unique_books['Year-Of-Publication'].astype(int)\n",
    "min_year = unique_books['Year-Of-Publication'].min()\n",
    "max_year = unique_books['Year-Of-Publication'].max()\n",
    "unique_books['Year-Of-Publication'] = (unique_books['Year-Of-Publication'] - min_year) / (max_year - min_year)\n",
    "\n",
    "# Convert to tensor\n",
    "book_features_tensor = torch.tensor(unique_books[['Book-Title', 'Book-Author', 'Publisher', 'Year-Of-Publication']].values, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['books'].x = book_features_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['books'].x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_indices = df['encoded-user-ID'].to_numpy()\n",
    "book_indices = book_key_encoder.transform(df['Book-Key'])\n",
    "\n",
    "# Create tensors for user indices, book indices, and ratings\n",
    "user_indices_tensor = torch.tensor(user_indices, dtype=torch.long)\n",
    "book_indices_tensor = torch.tensor(book_indices, dtype=torch.long)\n",
    "ratings_tensor = torch.tensor(df['Book-Rating'].values, dtype=torch.float)\n",
    "\n",
    "# Adding edge data (edges from 'user' to 'book' with a relationship 'rated')\n",
    "data['users', 'rated', 'books'].edge_index = torch.stack([user_indices_tensor, book_indices_tensor], dim=0)\n",
    "data['users', 'rated', 'books'].edge_attr = ratings_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['users', 'rated', 'books'].num_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get indices range for users and books\n",
    "edge_index = data['users', 'rated', 'books'].edge_index\n",
    "\n",
    "print(\"User indices range:\", edge_index[0].min().item(), edge_index[0].max().item())\n",
    "print(\"Book indices range:\", edge_index[1].min().item(), edge_index[1].max().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GNN Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeteroGNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, out_channels, num_layers):\n",
    "        super().__init__()\n",
    "        \n",
    "        # create SageConv layers\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            conv = HeteroConv({\n",
    "                ('user', 'rates', 'book'): SAGEConv((-1, -1), hidden_channels), # SAGEConv updates nodes based on neighbors\n",
    "            }, aggr='sum') # aggregate by sum \n",
    "            self.convs.append(conv)\n",
    "\n",
    "        self.lin = Linear(hidden_channels, out_channels) # linear transformation\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict):\n",
    "        for conv in self.convs:\n",
    "            x_dict = conv(x_dict, edge_index_dict) # apply conv layers\n",
    "            x_dict = {key: F.relu(x) for key, x in x_dict.items()}  # apply ReLU to the output of each layer\n",
    "        return self.lin(x_dict['user']) # pass final output through a linear layer\n",
    "\n",
    "# Base model\n",
    "model = HeteroGNN(hidden_channels=64, out_channels=1, num_layers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'data' is your HeteroData object and it has a 'book' key\n",
    "num_books = data['books'].x.size(0)  # Total number of books\n",
    "\n",
    "# Creating a mask with 80% of the data for training\n",
    "train_mask = torch.rand(num_books) < 0.8\n",
    "\n",
    "# Assigning the mask to your data object\n",
    "data['books'].train_mask = train_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Adam optimizer\n",
    "optimizer = Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "def train(batch):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(batch.x_dict, batch.edge_index_dict) # Forward pass\n",
    "    mask = batch['book'].train_mask  #  Use mask defined previously \n",
    "    loss = F.cross_entropy(out[mask], batch['book'].y[mask]) # Calculate loss\n",
    "    loss.backward() # Backward pass\n",
    "    optimizer.step() # Update model parameters\n",
    "    return float(loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = T.ToUndirected()  # Make sure edges are bidirectional.\n",
    "\n",
    "data = transform(data)  # Apply transformation.\n",
    "\n",
    "train_loader = NeighborLoader(\n",
    "    data,\n",
    "    num_neighbors=[15] * 2, # Sample 15 neighbors for each node and each edge type for 2 iterations:\n",
    "    batch_size=128,\n",
    "    input_nodes=('books', data['books'].train_mask),  \n",
    ")\n",
    "\n",
    "for batch in train_loader:\n",
    "    loss = train(batch)\n",
    "    print(f'Loss: {loss:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.5 ('env_GNN': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8f6377febe87314814c7b7161cea679ad38a9298f17e15a391a898e937fe96e0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
