{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempt at a GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing of our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = pd.read_csv(\"Data/Ratings.csv\")\n",
    "books = pd.read_csv(\"Data/Books.csv\", dtype={3: str})\n",
    "users = pd.read_csv(\"Data/Users.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 105283 unique users, and 340556 unique books in the ratings dataset.\n",
      "There are around 21.0% of books in the graph missing in the books data\n",
      "There are: 105283, who have rated at least one book\n"
     ]
    }
   ],
   "source": [
    "# We will only use users and books present in the ratings dataset \n",
    "lessen_user_ids = {userid: idx for idx, userid in enumerate(ratings['User-ID'].unique())} #renumber IDs to reduce inactive users\n",
    "ratings['New-User-ID'] = ratings['User-ID'].map(lessen_user_ids)\n",
    "user_ids = list(ratings['New-User-ID'].unique())\n",
    "num_users = len(set(user_ids))\n",
    "\n",
    "# Map book identifiers (ISBN) to a unique integer identifier for datatype compatibility of dgl\n",
    "isbn_to_id = {isbn: idx for idx, isbn in enumerate(ratings['ISBN'].unique())}\n",
    "ratings['Book-ID'] = ratings['ISBN'].map(isbn_to_id)\n",
    "book_ids = list(ratings['Book-ID'].unique())\n",
    "num_books = len(set(book_ids))\n",
    "\n",
    "print(f'There are {len(user_ids)} unique users, and {len(book_ids)} unique books in the ratings dataset.')\n",
    " \n",
    "# Remove users and books not included in the ratings dataset\n",
    "books['Book-ID'] = books['ISBN'].map(isbn_to_id)\n",
    "books_clean = books[books['Book-ID'].isin(book_ids)]\n",
    "books_clean_ids = books_clean['Book-ID'].unique()\n",
    "percent_books_missing = round((num_books-len(books_clean_ids))/num_books*100, 0)\n",
    "\n",
    "print(f'There are around {percent_books_missing}% of books in the graph missing in the books data')\n",
    "\n",
    "users['New-User-ID'] = users['User-ID'].map(lessen_user_ids)\n",
    "users_clean = users[users['New-User-ID'].isin(user_ids)]\n",
    "print(f\"There are: {len(users_clean['New-User-ID'])}, who have rated at least one book\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_with_book_titles = ratings.merge(books,on='ISBN')\n",
    "ratings_with_book_titles.drop(columns=[\"ISBN\",\"Image-URL-S\",\"Image-URL-M\"],axis=1,inplace=True)\n",
    "# Drop Age because tooo many missing values\n",
    "complete_df = ratings_with_book_titles.merge(users.drop(\"Age\", axis=1), on=\"User-ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['User-ID', 'Book-Rating', 'New-User-ID_x', 'Book-ID_x', 'Book-Title',\n",
      "       'Book-Author', 'Year-Of-Publication', 'Publisher', 'Image-URL-L',\n",
      "       'Book-ID_y', 'Location', 'New-User-ID_y'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "complete_df['Location'] = complete_df['Location'].str.split(',').str[-1].str.strip()\n",
    "print(complete_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "647294\n",
      "1031136\n"
     ]
    }
   ],
   "source": [
    "print((complete_df['Book-Rating'] == 0).sum())\n",
    "print(len(complete_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "383842\n"
     ]
    }
   ],
   "source": [
    "df = complete_df.loc[complete_df['Book-Rating'] != 0]\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print((df['Year-Of-Publication'] == 'DK Publishing Inc').sum())\n",
    "df = df[df['Year-Of-Publication'] != 'DK Publishing Inc']\n",
    "print((df['Year-Of-Publication'] == 'DK Publishing Inc').sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User-ID</th>\n",
       "      <th>Book-Rating</th>\n",
       "      <th>New-User-ID_x</th>\n",
       "      <th>Book-ID_x</th>\n",
       "      <th>Book-Title</th>\n",
       "      <th>Book-Author</th>\n",
       "      <th>Year-Of-Publication</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>Image-URL-L</th>\n",
       "      <th>Book-ID_y</th>\n",
       "      <th>Location</th>\n",
       "      <th>New-User-ID_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>276726</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Rites of Passage</td>\n",
       "      <td>Judith Rae</td>\n",
       "      <td>2001</td>\n",
       "      <td>Heinle</td>\n",
       "      <td>http://images.amazon.com/images/P/0155061224.0...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>usa</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>276729</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Help!: Level 1</td>\n",
       "      <td>Philip Prowse</td>\n",
       "      <td>1999</td>\n",
       "      <td>Cambridge University Press</td>\n",
       "      <td>http://images.amazon.com/images/P/052165615X.0...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>croatia</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>276729</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>The Amsterdam Connection : Level 4 (Cambridge ...</td>\n",
       "      <td>Sue Leather</td>\n",
       "      <td>2001</td>\n",
       "      <td>Cambridge University Press</td>\n",
       "      <td>http://images.amazon.com/images/P/0521795028.0...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>croatia</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>276744</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>A Painted House</td>\n",
       "      <td>JOHN GRISHAM</td>\n",
       "      <td>2001</td>\n",
       "      <td>Doubleday</td>\n",
       "      <td>http://images.amazon.com/images/P/038550120X.0...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>usa</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>276747</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>Little Altars Everywhere</td>\n",
       "      <td>Rebecca Wells</td>\n",
       "      <td>2003</td>\n",
       "      <td>HarperTorch</td>\n",
       "      <td>http://images.amazon.com/images/P/0060517794.0...</td>\n",
       "      <td>16.0</td>\n",
       "      <td>usa</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    User-ID  Book-Rating  New-User-ID_x  Book-ID_x  \\\n",
       "1    276726            5              1          1   \n",
       "3    276729            3              3          3   \n",
       "4    276729            6              3          4   \n",
       "6    276744            7              7          8   \n",
       "13   276747            9             10         16   \n",
       "\n",
       "                                           Book-Title    Book-Author  \\\n",
       "1                                    Rites of Passage     Judith Rae   \n",
       "3                                      Help!: Level 1  Philip Prowse   \n",
       "4   The Amsterdam Connection : Level 4 (Cambridge ...    Sue Leather   \n",
       "6                                     A Painted House   JOHN GRISHAM   \n",
       "13                           Little Altars Everywhere  Rebecca Wells   \n",
       "\n",
       "   Year-Of-Publication                   Publisher  \\\n",
       "1                 2001                      Heinle   \n",
       "3                 1999  Cambridge University Press   \n",
       "4                 2001  Cambridge University Press   \n",
       "6                 2001                   Doubleday   \n",
       "13                2003                 HarperTorch   \n",
       "\n",
       "                                          Image-URL-L  Book-ID_y Location  \\\n",
       "1   http://images.amazon.com/images/P/0155061224.0...        1.0      usa   \n",
       "3   http://images.amazon.com/images/P/052165615X.0...        3.0  croatia   \n",
       "4   http://images.amazon.com/images/P/0521795028.0...        4.0  croatia   \n",
       "6   http://images.amazon.com/images/P/038550120X.0...        8.0      usa   \n",
       "13  http://images.amazon.com/images/P/0060517794.0...       16.0      usa   \n",
       "\n",
       "    New-User-ID_y  \n",
       "1             1.0  \n",
       "3             3.0  \n",
       "4             3.0  \n",
       "6             7.0  \n",
       "13           10.0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(383841, 12)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68091\n",
      "135566\n"
     ]
    }
   ],
   "source": [
    "print(len(df['User-ID'].unique()))\n",
    "print(len(df['Book-Title'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start with graph structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maretreinders/Documents/3rd year AUC/TM/GNN/env_GNN/lib/python3.11/site-packages/threadpoolctl.py:1214: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.data import HeteroData\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = HeteroData()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique users\n",
    "unique_users = df[['User-ID', 'Location']].drop_duplicates()\n",
    "\n",
    "# Take care of the User-IDs\n",
    "user_id_encoder = LabelEncoder()\n",
    "df['encoded-user-ID'] = user_id_encoder.fit_transform(df['User-ID'].astype(str))    # Do so otherwise it is going to create a problem with tensor indexing later\n",
    "unique_users['User-ID'] = df['encoded-user-ID']\n",
    "\n",
    "# Encode the Location feature\n",
    "location_encoder = LabelEncoder()\n",
    "unique_users['Location'] = location_encoder.fit_transform(unique_users['Location'])\n",
    "\n",
    "# Get tensor\n",
    "user_features_tensor = torch.tensor(unique_users[['User-ID', 'Location']].values, dtype=torch.float)\n",
    "data['users'].x = user_features_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([68091, 2])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['users'].x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Book nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a composite key that uniquely identifies each book\n",
    "df['Book-Key'] = df['Book-Title'] + '|' + df['Book-Author'] + '|' + df['Publisher'] + '|' + df['Year-Of-Publication'].astype(str)\n",
    "\n",
    "# Now, get unique books using the new composite key\n",
    "unique_books = df[['Book-Key', 'Book-Title', 'Book-Author', 'Publisher', 'Year-Of-Publication']].drop_duplicates()\n",
    "\n",
    "# Encode categorical features\n",
    "book_key_encoder = LabelEncoder()\n",
    "unique_books['Book-Key'] = book_key_encoder.fit_transform(unique_books['Book-Key'])\n",
    "\n",
    "title_encoder = LabelEncoder()\n",
    "author_encoder = LabelEncoder()\n",
    "publisher_encoder = LabelEncoder()\n",
    "\n",
    "unique_books['Book-Title'] = title_encoder.fit_transform(unique_books['Book-Title'])\n",
    "unique_books['Book-Author'] = author_encoder.fit_transform(unique_books['Book-Author'])\n",
    "unique_books['Publisher'] = publisher_encoder.fit_transform(unique_books['Publisher'])\n",
    "\n",
    "# Normalize year of publication\n",
    "unique_books['Year-Of-Publication'] = unique_books['Year-Of-Publication'].astype(int)\n",
    "min_year = unique_books['Year-Of-Publication'].min()\n",
    "max_year = unique_books['Year-Of-Publication'].max()\n",
    "unique_books['Year-Of-Publication'] = (unique_books['Year-Of-Publication'] - min_year) / (max_year - min_year)\n",
    "\n",
    "# Convert to tensor\n",
    "book_features_tensor = torch.tensor(unique_books[['Book-Title', 'Book-Author', 'Publisher', 'Year-Of-Publication']].values, dtype=torch.float)\n",
    "data['books'].x = book_features_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([149243, 4])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['books'].x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_indices = df['encoded-user-ID'].to_numpy()\n",
    "book_indices = book_key_encoder.transform(df['Book-Key'])\n",
    "\n",
    "# Create tensors for user indices, book indices, and ratings\n",
    "user_indices_tensor = torch.tensor(user_indices, dtype=torch.long)\n",
    "book_indices_tensor = torch.tensor(book_indices, dtype=torch.long)\n",
    "ratings_tensor = torch.tensor(df['Book-Rating'].values, dtype=torch.float)\n",
    "\n",
    "# Adding edge data (edges from 'user' to 'book' with a relationship 'rated')\n",
    "data['users', 'rated', 'books'].edge_index = torch.stack([user_indices_tensor, book_indices_tensor], dim=0)\n",
    "data['users', 'rated', 'books'].edge_attr = ratings_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "383841"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['users', 'rated', 'books'].num_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HeteroData(\n",
      "  users={ x=[68091, 2] },\n",
      "  books={ x=[149243, 4] },\n",
      "  (users, rated, books)={\n",
      "    edge_index=[2, 383841],\n",
      "    edge_attr=[383841],\n",
      "  }\n",
      ")\n",
      "{('users', 'rated', 'books'): tensor([[ 47760,  47761,  47761,  ...,  47756,  47757,  47759],\n",
      "        [ 92341,  50886, 107531,  ...,  45852, 128517,  43549]])}\n",
      "tensor([ 5.,  3.,  6.,  ...,  9., 10., 10.])\n"
     ]
    }
   ],
   "source": [
    "print(data)\n",
    "print(data.edge_index_dict)\n",
    "print(data['users', 'rated', 'books'].edge_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User indices range: 0 68090\n",
      "Book indices range: 0 149239\n"
     ]
    }
   ],
   "source": [
    "edge_index = data['users', 'rated', 'books'].edge_index\n",
    "\n",
    "print(\"User indices range:\", edge_index[0].min().item(), edge_index[0].max().item())\n",
    "print(\"Book indices range:\", edge_index[1].min().item(), edge_index[1].max().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(len(data.x_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masks(num_nodes, train_frac=0.7, val_frac=0.15):\n",
    "    indices = np.random.permutation(num_nodes)\n",
    "    train_size = int(num_nodes * train_frac)\n",
    "    val_size = int(num_nodes * val_frac)\n",
    "\n",
    "    train_mask = np.zeros(num_nodes, dtype=bool)\n",
    "    val_mask = np.zeros(num_nodes, dtype=bool)\n",
    "    test_mask = np.zeros(num_nodes, dtype=bool)\n",
    "\n",
    "    train_mask[indices[:train_size]] = True\n",
    "    val_mask[indices[train_size:train_size + val_size]] = True\n",
    "    test_mask[indices[train_size + val_size:]] = True\n",
    "    \n",
    "    return torch.from_numpy(train_mask), torch.from_numpy(val_mask), torch.from_numpy(test_mask)\n",
    "\n",
    "# Create masks for users and books\n",
    "user_train_mask, user_val_mask, user_test_mask = create_masks(68091)\n",
    "book_train_mask, book_val_mask, book_test_mask = create_masks(149243)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['users'].train_mask = user_train_mask\n",
    "data['users'].val_mask = user_val_mask\n",
    "data['users'].test_mask = user_test_mask\n",
    "\n",
    "data['books'].train_mask = book_train_mask\n",
    "data['books'].val_mask = book_val_mask\n",
    "data['books'].test_mask = book_test_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HeteroData(\n",
      "  users={\n",
      "    x=[68091, 2],\n",
      "    train_mask=[68091],\n",
      "    val_mask=[68091],\n",
      "    test_mask=[68091],\n",
      "  },\n",
      "  books={\n",
      "    x=[149243, 4],\n",
      "    train_mask=[149243],\n",
      "    val_mask=[149243],\n",
      "    test_mask=[149243],\n",
      "  },\n",
      "  (users, rated, books)={\n",
      "    edge_index=[2, 383841],\n",
      "    edge_attr=[383841],\n",
      "  }\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import HeteroConv, SAGEConv, GATConv, Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = T.ToUndirected(merge=True)(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HeteroData(\n",
      "  users={\n",
      "    x=[68091, 2],\n",
      "    train_mask=[68091],\n",
      "    val_mask=[68091],\n",
      "    test_mask=[68091],\n",
      "  },\n",
      "  books={\n",
      "    x=[149243, 4],\n",
      "    train_mask=[149243],\n",
      "    val_mask=[149243],\n",
      "    test_mask=[149243],\n",
      "  },\n",
      "  (users, rated, books)={\n",
      "    edge_index=[2, 383841],\n",
      "    edge_attr=[383841],\n",
      "  },\n",
      "  (books, rev_rated, users)={\n",
      "    edge_index=[2, 383841],\n",
      "    edge_attr=[383841],\n",
      "  }\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'users': tensor([[-1.2565e+01],\n",
      "        [-1.2689e+04],\n",
      "        [-8.5426e+03],\n",
      "        ...,\n",
      "        [-1.4228e+04],\n",
      "        [-1.1571e+04],\n",
      "        [-8.0344e+03]]), 'books': tensor([[-308.0213],\n",
      "        [-501.9356],\n",
      "        [ -93.6838],\n",
      "        ...,\n",
      "        [-341.3477],\n",
      "        [ 486.0679],\n",
      "        [ 273.0971]])}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class HeteroGNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, out_channels, num_layers):\n",
    "        super().__init__()\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            conv = HeteroConv({\n",
    "                ('users', 'rated', 'books'): SAGEConv((-1, -1), hidden_channels),\n",
    "                ('books', 'rev_rated', 'users'): GATConv((-1, -1), hidden_channels, add_self_loops=False),\n",
    "            }, aggr='sum')\n",
    "            self.convs.append(conv)\n",
    "\n",
    "        self.user_lin = Linear(hidden_channels, out_channels)\n",
    "        self.book_lin = Linear(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict):\n",
    "        for conv in self.convs:\n",
    "            x_dict = conv(x_dict, edge_index_dict)\n",
    "            x_dict = {key: F.relu(x) for key, x in x_dict.items()}\n",
    "        return {\n",
    "            'users': self.user_lin(x_dict['users']),\n",
    "            'books': self.book_lin(x_dict['books'])\n",
    "        }\n",
    "\n",
    "model = HeteroGNN(hidden_channels=64, out_channels=1, num_layers=2)\n",
    "\n",
    "with torch.no_grad():  # Initialize lazy modules.\n",
    "    out = model(data.x_dict, data.edge_index_dict)\n",
    "    print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 30.586101531982422\n",
      "Epoch 2/20, Loss: 30.586101531982422\n",
      "Epoch 3/20, Loss: 30.586101531982422\n",
      "Epoch 4/20, Loss: 30.586101531982422\n",
      "Epoch 5/20, Loss: 30.586101531982422\n",
      "Epoch 6/20, Loss: 30.586101531982422\n",
      "Epoch 7/20, Loss: 30.586101531982422\n",
      "Epoch 8/20, Loss: 30.586101531982422\n",
      "Epoch 9/20, Loss: 30.586101531982422\n",
      "Epoch 10/20, Loss: 30.586101531982422\n",
      "Epoch 11/20, Loss: 30.586101531982422\n",
      "Epoch 12/20, Loss: 30.586101531982422\n",
      "Epoch 13/20, Loss: 30.586101531982422\n",
      "Epoch 14/20, Loss: 30.586101531982422\n",
      "Epoch 15/20, Loss: 30.586101531982422\n",
      "Epoch 16/20, Loss: 30.586101531982422\n",
      "Epoch 17/20, Loss: 30.586101531982422\n",
      "Epoch 18/20, Loss: 30.586101531982422\n",
      "Epoch 19/20, Loss: 30.586101531982422\n",
      "Epoch 20/20, Loss: 30.586101531982422\n"
     ]
    }
   ],
   "source": [
    "def train(num_epochs, data):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x_dict, data.edge_index_dict)\n",
    "        \n",
    "        # Calculate interaction scores\n",
    "        interaction_scores = torch.sigmoid((out['users'][data['users', 'rated', 'books'].edge_index[0]] * out['books'][data['users', 'rated', 'books'].edge_index[1]]).sum(dim=1))\n",
    "        \n",
    "        # Convert boolean tensor to float tensor for BCE loss\n",
    "        target_scores = (data['users', 'rated', 'books'].edge_attr > 0).float()\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = F.binary_cross_entropy(interaction_scores, target_scores)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}')\n",
    "\n",
    "num_epochs = 20\n",
    "train(num_epochs, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_edge_masks(data, relation, num_edges):\n",
    "    # This is a simple random split for illustration; adjust based on your needs\n",
    "    indices = torch.randperm(num_edges)\n",
    "    train_size = int(num_edges * 0.7)\n",
    "    val_size = int(num_edges * 0.15)\n",
    "\n",
    "    # Assuming you don't have these masks\n",
    "    if not hasattr(data[relation], 'train_mask'):\n",
    "        data[relation].train_mask = torch.zeros(num_edges, dtype=torch.bool)\n",
    "        data[relation].val_mask = torch.zeros(num_edges, dtype=torch.bool)\n",
    "        data[relation].test_mask = torch.zeros(num_edges, dtype=torch.bool)\n",
    "\n",
    "        data[relation].train_mask[indices[:train_size]] = True\n",
    "        data[relation].val_mask[indices[train_size:train_size + val_size]] = True\n",
    "        data[relation].test_mask[indices[train_size + val_size:]] = True\n",
    "\n",
    "# Example usage:\n",
    "assign_edge_masks(data, ('users', 'rated', 'books'), data[('users', 'rated', 'books')].edge_index.size(1))\n",
    "assign_edge_masks(data, ('books', 'rev_rated', 'users'), data[('books', 'rev_rated', 'users')].edge_index.size(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6960539113519522, Precision: 1.0, Recall: 0.6960539113519522, F1 Score: 0.8207922008765821\n"
     ]
    }
   ],
   "source": [
    "def evaluate(model, data, relation=('users', 'rated', 'books')):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data.x_dict, data.edge_index_dict)\n",
    "\n",
    "        # Ensure the mask is for edges, not nodes\n",
    "        if 'val_mask' not in data[relation]:\n",
    "            raise ValueError(\"Validation mask for edges not found in data!\")\n",
    "        \n",
    "        val_mask = data[relation].val_mask\n",
    "        \n",
    "        # Use the validation mask to filter the edge indices for users and books\n",
    "        test_user_indices = data[relation].edge_index[0][val_mask]\n",
    "        test_book_indices = data[relation].edge_index[1][val_mask]\n",
    "        \n",
    "        # Calculate interaction scores based on the filtered indices\n",
    "        user_scores = out['users'][test_user_indices]\n",
    "        book_scores = out['books'][test_book_indices]\n",
    "        interaction_scores = torch.sigmoid((user_scores * book_scores).sum(dim=1))\n",
    "        \n",
    "        # True labels for the test data\n",
    "        true_labels = (data[relation].edge_attr[val_mask] > 0).float()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        predicted_labels = interaction_scores > 0.5  # Thresholding at 0.5\n",
    "        accuracy = accuracy_score(true_labels.cpu().numpy(), predicted_labels.cpu().numpy())\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(true_labels.cpu().numpy(), predicted_labels.cpu().numpy(), average='binary')\n",
    "        \n",
    "        return accuracy, precision, recall, f1\n",
    "\n",
    "# Run evaluation\n",
    "accuracy, precision, recall, f1 = evaluate(model, data)\n",
    "print(f\"Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1 Score: {f1}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.5 ('env_GNN': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8f6377febe87314814c7b7161cea679ad38a9298f17e15a391a898e937fe96e0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
