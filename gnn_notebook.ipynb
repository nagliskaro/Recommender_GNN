{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Book Recommender System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For Pre-Processing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "\n",
    "# For GNN\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import networkx as nx\n",
    "import dgl\n",
    "import dgl.function as fn\n",
    "from dgl.nn import GraphConv\n",
    "from dgl.nn import HeteroGNNExplainer\n",
    "import dgl.nn.pytorch as dglnn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0f/xqjk7pr56mjd12xvy1w435_40000gn/T/ipykernel_5501/3661012832.py:2: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  books = pd.read_csv(\"Data/Books.csv\")\n"
     ]
    }
   ],
   "source": [
    "ratings = pd.read_csv(\"Data/Ratings.csv\")\n",
    "books = pd.read_csv(\"Data/Books.csv\")\n",
    "users = pd.read_csv(\"Data/Users.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "- We will only use users and books present in the ratings dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_country(location):\n",
    "    if not location:\n",
    "        return None\n",
    "    parts = [part.strip() for part in location.split(',')]\n",
    "    \n",
    "    return parts[-1] if parts and parts[-1] else None\n",
    "\n",
    "users['Country'] = users['Location'].apply(extract_country)\n",
    "\n",
    "# label_encoder = LabelEncoder()\n",
    "# users['Country'] = label_encoder.fit_transform(users['Country'])\n",
    "\n",
    "# Removing Book Ratings that have a 0 rating\n",
    "ratings = ratings[ratings['Book-Rating'] > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering for Books and Users that have a rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of User IDs in raitngs: 77805\n",
      "There are: 185973, unique book IDs\n",
      "There are 77805 unique users, and 185973 unique books in the ratings dataset.\n",
      "\n",
      "There are: 149836, books that have an ISBN\n",
      "There are around 19.0% of books in the graph missing in the books data\n",
      "There are: 77805, who have rated at least one book\n"
     ]
    }
   ],
   "source": [
    "# Renaming User IDs\n",
    "rename_user_ids = {userid: idx for idx, userid in enumerate(ratings['User-ID'].unique())}\n",
    "# Mapping new User IDs to Users that have a rating\n",
    "ratings['New-User-ID'] = ratings['User-ID'].map(rename_user_ids)\n",
    "# Getting the unique User IDs Ratings\n",
    "ratings_user_ids = list(ratings['New-User-ID'].unique())\n",
    "print(f\"Number of User IDs in raitngs: {len(ratings_user_ids)}\")\n",
    "\n",
    "# ISBN in Ratings Data sets\n",
    "isbn_to_id = {isbn: idx for idx, isbn in enumerate(ratings['ISBN'].unique())}\n",
    "# Map new ISBN to Books\n",
    "ratings['New-Book-ISBN'] = ratings['ISBN'].map(isbn_to_id)\n",
    "# Get unique Book Ratings \n",
    "ratings_book_ids = list(ratings['New-Book-ISBN'].unique())\n",
    "print(f\"There are: {len(ratings_book_ids)}, unique book IDs\")\n",
    "\n",
    "print(f'There are {len(ratings_user_ids)} unique users, and {len(ratings_book_ids)} unique books in the ratings dataset.\\n')\n",
    "\n",
    "# ===========================================================================================================================\n",
    "# ===========================================================================================================================\n",
    "\n",
    "# ISBN in Books Data set\n",
    "books['New-Book-ISBN'] = books['ISBN'].map(isbn_to_id)\n",
    "# Filtering for books that have a rating\n",
    "books_clean = books[books['New-Book-ISBN'].isin(ratings_book_ids)]\n",
    "\n",
    "print(f\"There are: {len(books_clean['New-Book-ISBN'].unique())}, books that have an ISBN\")\n",
    "\n",
    "books_clean_ids = books_clean['New-Book-ISBN'].unique()\n",
    "percent_books_missing = round((len(ratings_book_ids)-len(books_clean_ids))/len(ratings_book_ids)*100, 0)\n",
    "\n",
    "print(f'There are around {percent_books_missing}% of books in the graph missing in the books data')\n",
    "\n",
    "users['New-User-ID'] = users['User-ID'].map(rename_user_ids)\n",
    "users_clean = users[users['New-User-ID'].isin(ratings_user_ids)]\n",
    "print(f\"There are: {len(users_clean['New-User-ID'])}, who have rated at least one book\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aroung 1/5 of the books that have rating information do not have further information on the books dataset. However, as our objective is to investigate an user-based recommender system, this is irrelevant. We are able to embed the age and location data of users. As the age data is sparse, location data will be our main source of information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User-ID</th>\n",
       "      <th>ISBN</th>\n",
       "      <th>Book-Rating</th>\n",
       "      <th>New-User-ID</th>\n",
       "      <th>New-Book-ISBN</th>\n",
       "      <th>AVG_Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>276726</td>\n",
       "      <td>0155061224</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>276729</td>\n",
       "      <td>052165615X</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>276729</td>\n",
       "      <td>0521795028</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>276736</td>\n",
       "      <td>3257224281</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>6.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>276737</td>\n",
       "      <td>0600570967</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>6.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   User-ID        ISBN  Book-Rating  New-User-ID  New-Book-ISBN  AVG_Rating\n",
       "0   276726  0155061224            5            0              0        5.00\n",
       "1   276729  052165615X            3            1              1        3.00\n",
       "2   276729  0521795028            6            1              2        6.00\n",
       "3   276736  3257224281            8            2              3        6.75\n",
       "4   276737  0600570967            6            3              4        6.00"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Including average rating\n",
    "avg_rating = ratings.groupby(\"New-Book-ISBN\")[\"Book-Rating\"].mean().reset_index()\n",
    "avg_rating = avg_rating.rename(columns={'Book-Rating': 'AVG_Rating'})\n",
    "\n",
    "ratings = ratings.merge(avg_rating, on=\"New-Book-ISBN\", how='left')\n",
    "\n",
    "ratings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fill in missing value Age with simple imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0f/xqjk7pr56mjd12xvy1w435_40000gn/T/ipykernel_5501/4026430534.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  users_clean['Age'] = knn_imputer.fit_transform(users_clean[['Age']])\n"
     ]
    }
   ],
   "source": [
    "knn_imputer = KNNImputer(n_neighbors=5)\n",
    "users_clean['Age'] = knn_imputer.fit_transform(users_clean[['Age']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "433671 433671\n",
      "There are 77805 users, and 185973 books\n",
      "Graph(num_nodes={'book': 185973, 'user': 77805},\n",
      "      num_edges={('user', 'rating', 'book'): 433671},\n",
      "      metagraph=[('user', 'book', 'rating')])\n"
     ]
    }
   ],
   "source": [
    "src_tensor = torch.tensor(ratings['New-User-ID'].values)\n",
    "dst_tensor = torch.tensor(ratings['New-Book-ISBN'].values)\n",
    "\n",
    "# Users and Books datasets are metadata\n",
    "\n",
    "print(len(src_tensor), len(dst_tensor))\n",
    "\n",
    "num_users = len(ratings_user_ids)\n",
    "num_books = len(ratings_book_ids)\n",
    "print(f\"There are {num_users} users, and {num_books} books\")\n",
    "\n",
    "# # Initialize the adjacency matrix with zeros\n",
    "# adjacency_matrix = np.zeros((num_users, num_books))\n",
    "# # Populate the adjacency matrix\n",
    "# for user, book, rating in zip(src_tensor, dst_tensor, ratings_values):\n",
    "#     adjacency_matrix[user, book] = rating\n",
    "# print(f\"Size of the adjacency matrix: {adjacency_matrix.shape}\")\n",
    "\n",
    "# Dictionary which defines the Heterograph\n",
    "edges = {\n",
    "    ('user', 'rating', 'book'): (src_tensor, dst_tensor)\n",
    "}\n",
    "g = dgl.heterograph(edges, num_nodes_dict={'user': num_users, 'book': num_books})\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weigth the edges by ratings\n",
    "rating_data = ratings['Book-Rating'].values\n",
    "g.edges['rating'].data['rating'] = torch.tensor(rating_data, dtype=torch.float32) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add age to user feature\n",
    "ages = users_clean.set_index('New-User-ID')['Age'].sort_index().values\n",
    "g.nodes['user'].data['age'] =  torch.tensor(ages, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extract the country from the location by obtaining the expression after the last comma in e.g. nyc, new york, usa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0f/xqjk7pr56mjd12xvy1w435_40000gn/T/ipykernel_5501/1079567941.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  users_clean['Country'] = users_clean['Location'].str.rsplit(',', n=1).str[-1].str.strip()\n"
     ]
    }
   ],
   "source": [
    "# Extracting only the Country from the Location\n",
    "users_clean['Country'] = users_clean['Location'].str.rsplit(',', n=1).str[-1].str.strip()\n",
    "# Country Frequency\n",
    "country_counts = users_clean['Country'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that less frequent locations do not always contain country names, so we remove values of locations representing less than 1%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "rare_countries = country_counts[country_counts < 0.01].index\n",
    "users_clean.loc[users_clean['Country'].isin(rare_countries), 'Country'] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding Countries to a unique interger (same as label encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0f/xqjk7pr56mjd12xvy1w435_40000gn/T/ipykernel_5501/3859667754.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  users_clean['CountryId'] = users_clean['Country'].map(country_ids).fillna(-1).astype(int)\n"
     ]
    }
   ],
   "source": [
    "# country_ids = {country: idx for idx, country in enumerate(users_clean['Country'].unique())}  # map country to a unique integer\n",
    "# users_clean['CountryId'] = users_clean['Country'].map(country_ids)\n",
    "\n",
    "country_ids = {country: idx for idx, country in enumerate(users_clean['Country'].dropna().unique())}\n",
    "users_clean['CountryId'] = users_clean['Country'].map(country_ids).fillna(-1).astype(int)\n",
    "\n",
    "countries_value_count = users_clean['CountryId'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountryId\n",
      " 1    45368\n",
      " 0     6986\n",
      "-1     6273\n",
      " 4     4445\n",
      " 6     4040\n",
      " 2     2643\n",
      " 8     2499\n",
      " 3     2386\n",
      " 7     2191\n",
      " 5      974\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(countries_value_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = users_clean['CountryId'].values\n",
    "countries_tensor = torch.tensor(countries, dtype=torch.float32)\n",
    "g.nodes['user'].data['country'] = countries_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding Average Ratings as Meta Data to books nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_book_ratings = ratings.drop_duplicates(subset='New-Book-ISBN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding Average Books\n",
    "book_avg_rating = unique_book_ratings[\"AVG_Rating\"]\n",
    "avg_rating_tensor = torch.tensor(book_avg_rating, dtype=torch.float32)\n",
    "g.nodes['book'].data['AVG_Rating'] = avg_rating_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic graph info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NodeSpace(data={'age': tensor([35.8179, 16.0000, 35.8179,  ..., 38.0000, 14.0000, 12.0000]), 'country': tensor([0., 1., 2.,  ..., 1., 8., 1.])})\n",
      "NodeSpace(data={'AVG_Rating': tensor([5., 3., 6.,  ..., 5., 5., 8.])})\n"
     ]
    }
   ],
   "source": [
    "print(g.nodes['user'])\n",
    "print(g.nodes['book'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph(num_nodes={'book': 185973, 'user': 77805},\n",
      "      num_edges={('user', 'rating', 'book'): 433671},\n",
      "      metagraph=[('user', 'book', 'rating')])\n",
      "Number of users: 77805\n",
      "Number of books: 185973\n",
      "Number of ratings: 433671\n"
     ]
    }
   ],
   "source": [
    "print(g)  # Prints the basic info of the graph, such as number of nodes and edges per type\n",
    "\n",
    "# Print number of nodes for each type\n",
    "print(\"Number of users:\", g.number_of_nodes('user'))\n",
    "print(\"Number of books:\", g.number_of_nodes('book'))\n",
    "\n",
    "# Print number of edges\n",
    "print(\"Number of ratings:\", g.number_of_edges('rating'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node and Edge feature inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User features: dict_keys(['age', 'country'])\n",
      "Book features: dict_keys(['AVG_Rating'])\n",
      "Edge features: dict_keys(['rating'])\n",
      "Sample user ages: tensor([35.8179, 16.0000, 35.8179, 14.0000, 35.8179])\n",
      "Sample ratings: tensor([5., 3., 6., 8., 6.])\n"
     ]
    }
   ],
   "source": [
    "# Print user node features\n",
    "print(\"User features:\", g.nodes['user'].data.keys())\n",
    "\n",
    "# Print book node features, if any\n",
    "print(\"Book features:\", g.nodes['book'].data.keys())\n",
    "\n",
    "# Print edge features\n",
    "print(\"Edge features:\", g.edges['rating'].data.keys())\n",
    "\n",
    "# Example to print specific feature details:\n",
    "print(\"Sample user ages:\", g.nodes['user'].data['age'][:5])  # prints first 5 user ages\n",
    "print(\"Sample ratings:\", g.edges['rating'].data['rating'][:5])  # prints first 5 ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eliminate isolated nodes if any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "compact_g = dgl.compact_graphs(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create synthetic features for book based on degree of the node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_in_degrees = compact_g.in_degrees(etype=('user', 'rating', 'book')).float().unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "compact_g.nodes['book'].data['in_degree'] = book_in_degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = compact_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CIAO' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mCIAO\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'CIAO' is not defined"
     ]
    }
   ],
   "source": [
    "CIAO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the GNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NodeSpace(data={'age': tensor([35.8179, 16.0000, 35.8179,  ..., 38.0000, 14.0000, 12.0000]), 'country': tensor([0., 1., 2.,  ..., 1., 8., 1.]), '_ID': tensor([    0,     1,     2,  ..., 77802, 77803, 77804])})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.nodes['user']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNRecommender(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_feats, out_feats):\n",
    "        super(GNNRecommender, self).__init__()\n",
    "        self.user_embedding = nn.Embedding(num_users, in_feats)\n",
    "        self.book_embedding = nn.Embedding(num_books, in_feats)\n",
    "        self.user_conv1 = GraphConv(in_feats, hidden_feats, allow_zero_in_degree=True)\n",
    "        self.user_conv2 = GraphConv(hidden_feats, out_feats, allow_zero_in_degree=True)\n",
    "        self.book_conv1 = GraphConv(in_feats, hidden_feats, allow_zero_in_degree=True)\n",
    "        self.book_conv2 = GraphConv(hidden_feats, out_feats, allow_zero_in_degree=True)\n",
    "        self.fc = nn.Linear(out_feats * 2, 1)\n",
    "\n",
    "    def forward(self, g, user_ids, book_ids):\n",
    "        # Get initial node features\n",
    "        user_feats = self.user_embedding(torch.arange(num_users))\n",
    "        book_feats = self.book_embedding(torch.arange(num_books))\n",
    "\n",
    "        # Assign initial features to graph\n",
    "        g.nodes['user'].data['h'] = user_feats\n",
    "        g.nodes['book'].data['h'] = book_feats\n",
    "\n",
    "        # Apply graph convolution separately for users and books\n",
    "        g.nodes['user'].data['h'] = torch.relu(self.user_conv1(g, g.nodes['user'].data['h']))\n",
    "        g.nodes['user'].data['h'] = torch.relu(self.user_conv2(g, g.nodes['user'].data['h']))\n",
    "\n",
    "        g.nodes['book'].data['h'] = torch.relu(self.book_conv1(g, g.nodes['book'].data['h']))\n",
    "        g.nodes['book'].data['h'] = torch.relu(self.book_conv2(g, g.nodes['book'].data['h']))\n",
    "\n",
    "        # Get the updated node features\n",
    "        user_feats = g.nodes['user'].data['h']\n",
    "        book_feats = g.nodes['book'].data['h']\n",
    "\n",
    "        # Concatenate user and book features for final prediction\n",
    "        final_feats = torch.cat([user_feats[user_ids], book_feats[book_ids]], dim=1)\n",
    "\n",
    "        # Predict rating\n",
    "        rating_pred = self.fc(final_feats)\n",
    "        return rating_pred.squeeze()\n",
    "\n",
    "# Hyperparameters\n",
    "in_feats = 64\n",
    "hidden_feats = 128\n",
    "out_feats = 64\n",
    "\n",
    "model = GNNRecommender(in_feats, hidden_feats, out_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GNNRecommender(\n",
       "  (user_embedding): Embedding(77805, 64)\n",
       "  (book_embedding): Embedding(185973, 64)\n",
       "  (conv1): GraphConv(in=64, out=128, normalization=both, activation=None)\n",
       "  (conv2): GraphConv(in=128, out=64, normalization=both, activation=None)\n",
       "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from dgl.nn import GraphConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "DGLError",
     "evalue": "Expect number of features to match number of nodes (len(u)). Got 185973 and 77805 instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDGLError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdst_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[1;32m     15\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(predictions, ratings)\n",
      "File \u001b[0;32m~/miniconda3/envs/TextMining/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/TextMining/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[58], line 22\u001b[0m, in \u001b[0;36mGNNRecommender.forward\u001b[0;34m(self, g, user_ids, book_ids)\u001b[0m\n\u001b[1;32m     19\u001b[0m g\u001b[38;5;241m.\u001b[39mnodes[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbook\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mh\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m book_feats\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Apply graph convolution separately for users and books\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[43mg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnodes\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mh\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_conv1(g, g\u001b[38;5;241m.\u001b[39mnodes[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mh\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[1;32m     23\u001b[0m g\u001b[38;5;241m.\u001b[39mnodes[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mh\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_conv2(g, g\u001b[38;5;241m.\u001b[39mnodes[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mh\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[1;32m     25\u001b[0m g\u001b[38;5;241m.\u001b[39mnodes[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbook\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mh\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbook_conv1(g, g\u001b[38;5;241m.\u001b[39mnodes[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbook\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mh\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n",
      "File \u001b[0;32m~/miniconda3/envs/TextMining/lib/python3.11/site-packages/dgl/view.py:99\u001b[0m, in \u001b[0;36mHeteroNodeDataView.__setitem__\u001b[0;34m(self, key, val)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(val, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, (\n\u001b[1;32m     96\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe HeteroNodeDataView has only one node type. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     97\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplease pass a tensor directly\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     98\u001b[0m     )\n\u001b[0;32m---> 99\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_graph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_n_repr\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ntid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mval\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/TextMining/lib/python3.11/site-packages/dgl/heterograph.py:4344\u001b[0m, in \u001b[0;36mDGLGraph._set_n_repr\u001b[0;34m(self, ntid, u, data)\u001b[0m\n\u001b[1;32m   4342\u001b[0m nfeats \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mshape(val)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   4343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nfeats \u001b[38;5;241m!=\u001b[39m num_nodes:\n\u001b[0;32m-> 4344\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DGLError(\n\u001b[1;32m   4345\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpect number of features to match number of nodes (len(u)).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4346\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Got \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (nfeats, num_nodes)\n\u001b[1;32m   4347\u001b[0m     )\n\u001b[1;32m   4348\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m F\u001b[38;5;241m.\u001b[39mcontext(val) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice:\n\u001b[1;32m   4349\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DGLError(\n\u001b[1;32m   4350\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCannot assign node feature \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m on device \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m to a graph on\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   4351\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m device \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. Call DGLGraph.to() to copy the graph to the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4352\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m same device.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(key, F\u001b[38;5;241m.\u001b[39mcontext(val), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   4353\u001b[0m     )\n",
      "\u001b[0;31mDGLError\u001b[0m: Expect number of features to match number of nodes (len(u)). Got 185973 and 77805 instead."
     ]
    }
   ],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 1\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    predictions = model(g, src_tensor, dst_tensor)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = criterion(predictions, ratings)\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define node features (for simplicity, we use random features here, replace with actual features if available)\n",
    "user_features = torch.randn(num_users, 10)  # 10-dimensional features for users\n",
    "book_features = torch.randn(num_books, 10)  # 10-dimensional features for books\n",
    "\n",
    "# Combine features into a single tensor, while keeping track of user and book indices\n",
    "features = torch.cat([user_features, book_features], dim=0)\n",
    "\n",
    "# Create and train the model\n",
    "model = LinkPredictor(in_feats=10, hidden_feats=16, out_feats=8)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    scores = model(g, features)\n",
    "    \n",
    "    # Assuming we have ground truth scores\n",
    "    ground_truth = ratings_values\n",
    "    \n",
    "    loss = loss_fn(scores, ground_truth)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f'Epoch {epoch}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine user and book features into tensors\n",
    "user_features = torch.cat([g.nodes['user'].data['age'].unsqueeze(1), g.nodes['user'].data['country'].unsqueeze(1)], dim=1)\n",
    "book_features = torch.cat([g.nodes['book'].data['AVG_Rating'].unsqueeze(1), g.nodes['book'].data['in_degree']], dim=1)\n",
    "\n",
    "# Edge features (ratings)\n",
    "edge_weights = g.edges['rating'].data['rating']\n",
    "\n",
    "# Define input feature dimensions and number of classes\n",
    "user_input_dim = user_features.shape[1]\n",
    "book_input_dim = book_features.shape[1]\n",
    "h_feats = 16  # Number of hidden features\n",
    "out_feats = 1  # Regression task (predicting ratings)\n",
    "\n",
    "# Initialize the model\n",
    "model = HeteroGNN(user_input_dim, book_input_dim, h_feats, out_feats)\n",
    "\n",
    "# Print model architecture\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training settings\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_fn = nn.MSELoss()  # Mean Squared Error for regression tasks\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 2\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    user_logits, book_logits = model(g, user_features, book_features, edge_weights)  # Forward pass\n",
    "\n",
    "    # Combine user and book embeddings to predict ratings\n",
    "    user_emb = user_logits[g.edges(etype='rating')[0]]\n",
    "    book_emb = book_logits[g.edges(etype='rating')[1]]\n",
    "    predicted_ratings = (user_emb + book_emb).mean(dim=1)  # Simplistic approach for combining embeddings\n",
    "    loss = loss_fn(predicted_ratings, g.edges['rating'].data['rating'])\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}/{num_epochs}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGED g. to compact_g.\n",
    "\n",
    "# Create USER Features Tensor\n",
    "age_tensor = compact_g.nodes['user'].data['age'].unsqueeze(1)\n",
    "country_tensor = compact_g.nodes['user'].data['country'].unsqueeze(1)\n",
    "user_feats = torch.cat([age_tensor, country_tensor], dim=1)\n",
    "\n",
    "# Create BOOK Features Tensor\n",
    "book_feats = compact_g.nodes['book'].data['in_degree']\n",
    "\n",
    "user_feat_dim = user_feats.shape[1]  # the size of user feature\n",
    "book_feat_dim = book_feats.shape[1]  # the size of book feature\n",
    "\n",
    "print(f\"User feature dimension {user_feat_dim}\")\n",
    "print(f\"Book feature dimension {book_feat_dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 32\n",
    "num_classes = 1  # predicting a single rating value\n",
    "model = GNNRecommender(user_feat_dim, book_feat_dim, hidden_size, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split graph for training and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_graph(g, proportion=0.8):\n",
    "    # Split edges randomly for training and validation\n",
    "    num_edges = g.number_of_edges('rating')\n",
    "    all_edges = np.arange(num_edges)\n",
    "    np.random.shuffle(all_edges)\n",
    "    \n",
    "    train_size = int(num_edges * proportion)\n",
    "    train_edges = all_edges[:train_size]\n",
    "    val_edges = all_edges[train_size:]\n",
    "    \n",
    "    # # Create subgraphs based on the edges (change to True)\n",
    "    g_train = dgl.edge_subgraph(g, train_edges, relabel_nodes=True)\n",
    "    g_val = dgl.edge_subgraph(g, val_edges, relabel_nodes=True)\n",
    "    \n",
    "    # Create subgraphs based on the edges\n",
    "    # g_train = dgl.edge_subgraph(g, {'rating': train_edges}, relabel_nodes=False)\n",
    "    # g_val = dgl.edge_subgraph(g, {'rating': val_edges}, relabel_nodes=False)\n",
    "    \n",
    "    return g_train, g_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_train, g_val = split_graph(compact_g, proportion=0.8)\n",
    "\n",
    "# Verify subgraphs\n",
    "print(\"Number of users in training graph:\", g_train.number_of_nodes('user'))\n",
    "print(\"Number of books in training graph:\", g_train.number_of_nodes('book'))\n",
    "print(\"Number of ratings in training graph:\", g_train.number_of_edges('rating'), \"\\n\")\n",
    "\n",
    "print(\"Number of users in validation graph:\", g_val.number_of_nodes('user'))\n",
    "print(\"Number of books in validation graph:\", g_val.number_of_nodes('book'))\n",
    "print(\"Number of ratings in validation graph:\", g_val.number_of_edges('rating'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the structure of the edge data\n",
    "print(g_train.edges['rating'].data)\n",
    "print(g_val.edges['rating'].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the features and ratings for the Training Set\n",
    "age_tensor_train = g_train.nodes['user'].data['age'].unsqueeze(1) # (N, 1)\n",
    "country_tensor_train = g_train.nodes['user'].data['country'].unsqueeze(1) # (N, 1)\n",
    "\n",
    "assert age_tensor_train.shape[0] == country_tensor_train.shape[0], \"Mismatch in number of users\"\n",
    "\n",
    "\n",
    "# Training Set\n",
    "user_features_train = torch.cat([age_tensor_train, country_tensor_train], dim=1) # (N, 2)\n",
    "book_features_train = g_train.nodes['book'].data['in_degree'] # (M, 1)\n",
    "\n",
    "ratings_train = g_train.edges['rating'].data['rating']\n",
    "\n",
    "# Add user and book features\n",
    "g_train.nodes['user'].data['features'] = user_features_train\n",
    "g_train.nodes['book'].data['features'] = book_features_train\n",
    "\n",
    "\n",
    "# Get the features and ratings for the Validation Set\n",
    "age_tensor_val = g_val.nodes['user'].data['age'].unsqueeze(1) # (N_val, 1)\n",
    "country_tensor_val = g_val.nodes['user'].data['country'].unsqueeze(1) # (N_val, 1)\n",
    "\n",
    "assert age_tensor_val.shape[0] == country_tensor_val.shape[0], \"Mismatch in number of validation users\"\n",
    "\n",
    "# Validation Set\n",
    "user_features_val = torch.cat([age_tensor_val, country_tensor_val], dim=1) # (N_val, 2)\n",
    "book_features_val = g_val.nodes['book'].data['in_degree'] # (M_val, 1)\n",
    "ratings_val = g_val.edges['rating'].data['rating']\n",
    "\n",
    "# Verify feature dimensions\n",
    "print(\"User features train shape:\", user_features_train.shape)\n",
    "print(\"Book features train shape:\", book_features_train.shape)\n",
    "print(\"Ratings train shape:\", ratings_train.shape, \"\\n\")\n",
    "\n",
    "print(\"User features val shape:\", user_features_val.shape)\n",
    "print(\"Book features val shape:\", book_features_val.shape)\n",
    "print(\"Ratings val shape:\", ratings_val.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_train.edges['rating'].data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, g, user_features, book_features, labels, optimizer, criterion):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(g, user_features, book_features)\n",
    "    # Added .squeeze()\n",
    "    loss = criterion(outputs.squeeze(), labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 2  # or however many epochs you deem necessary\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    loss = train(model, g_train, user_features_train, book_features_train, ratings_train, optimizer, criterion)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, g, user_features, book_features, labels, criterion):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(g, user_features, book_features)\n",
    "        # Added .squeeze()\n",
    "        loss = criterion(outputs.squeeze(), labels)\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation loss\n",
    "validation_loss = evaluate(model, g_val, user_features_val, book_features_val, ratings_val, criterion)\n",
    "print(f'Validation Loss: {validation_loss}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "8f6377febe87314814c7b7161cea679ad38a9298f17e15a391a898e937fe96e0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
