{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Book Recommender System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For Pre-Processing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# For GNN\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import dgl\n",
    "import dgl.function as fn\n",
    "from dgl.nn import GraphConv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratings Data Shape: (1149780, 3) \n",
      "\n",
      "User-ID         int64\n",
      "ISBN           object\n",
      "Book-Rating     int64\n",
      "dtype: object\n",
      "\n",
      "   User-ID        ISBN  Book-Rating\n",
      "0   276725  034545104X            0\n",
      "1   276726  0155061224            5\n",
      "2   276727  0446520802            0\n",
      "3   276729  052165615X            3\n",
      "4   276729  0521795028            6\n"
     ]
    }
   ],
   "source": [
    "ratings = pd.read_csv(\"Data/Ratings.csv\")\n",
    "print(f\"Ratings Data Shape: {ratings.shape} \\n\")\n",
    "print(f\"{ratings.dtypes}\\n\")\n",
    "print(ratings.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Books Data Shape: (271360, 8) \n",
      "\n",
      "ISBN                   object\n",
      "Book-Title             object\n",
      "Book-Author            object\n",
      "Year-Of-Publication    object\n",
      "Publisher              object\n",
      "Image-URL-S            object\n",
      "Image-URL-M            object\n",
      "Image-URL-L            object\n",
      "dtype: object\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0f/xqjk7pr56mjd12xvy1w435_40000gn/T/ipykernel_87175/2638443579.py:1: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  books = pd.read_csv(\"Data/Books.csv\")\n"
     ]
    }
   ],
   "source": [
    "books = pd.read_csv(\"Data/Books.csv\")\n",
    "print(f\"Books Data Shape: {books.shape} \\n\")\n",
    "print(f\"{books.dtypes}\\n\")\n",
    "# print(books.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nan-values by column\n",
      "ISBN                   0\n",
      "Book-Title             0\n",
      "Book-Author            2\n",
      "Year-Of-Publication    0\n",
      "Publisher              2\n",
      "Image-URL-S            0\n",
      "Image-URL-M            0\n",
      "Image-URL-L            3\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('Nan-values by column')\n",
    "print(books.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users Data Shape: (278858, 3) \n",
      "\n",
      "   User-ID                            Location   Age\n",
      "0        1                  nyc, new york, usa   NaN\n",
      "1        2           stockton, california, usa  18.0\n",
      "2        3     moscow, yukon territory, russia   NaN\n",
      "3        4           porto, v.n.gaia, portugal  17.0\n",
      "4        5  farnborough, hants, united kingdom   NaN\n",
      "\n",
      "User-ID       int64\n",
      "Location     object\n",
      "Age         float64\n",
      "dtype: object\n",
      "\n",
      "Nan-values by column\n",
      "User-ID          0\n",
      "Location         0\n",
      "Age         110762\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "users = pd.read_csv(\"Data/Users.csv\")\n",
    "print(f\"Users Data Shape: {users.shape} \\n\")\n",
    "\n",
    "print(users.head())\n",
    "print()\n",
    "print(f\"{users.dtypes}\\n\")\n",
    "\n",
    "print('Nan-values by column')\n",
    "print(users.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "- We will only use users and books present in the ratings dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Renumber IDs to reduce inactive users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# renumber IDs to reduce inactive users\n",
    "lessen_user_ids = {userid: idx for idx, userid in enumerate(ratings['User-ID'].unique())}\n",
    "ratings['New-User-ID'] = ratings['User-ID'].map(lessen_user_ids)\n",
    "user_ids = list(ratings['New-User-ID'].unique())\n",
    "num_users = len(set(user_ids))\n",
    "print(f\"Number of User IDs: {num_users}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map book identifiers (ISBN) to a unique integer identifier for datatype compatibility of dgl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map book identifiers (ISBN) to a unique integer identifier for datatype compatibility of dgl\n",
    "isbn_to_id = {isbn: idx for idx, isbn in enumerate(ratings['ISBN'].unique())}\n",
    "ratings['Book-ID'] = ratings['ISBN'].map(isbn_to_id)\n",
    "book_ids = list(ratings['Book-ID'].unique())\n",
    "num_books = len(set(book_ids))\n",
    "\n",
    "print(f'There are {len(user_ids)} unique users, and {len(book_ids)} unique books in the ratings dataset.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove users and books not included in the ratings dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove users and books not included in the ratings dataset\n",
    "books['Book-ID'] = books['ISBN'].map(isbn_to_id)\n",
    "books_clean = books[books['Book-ID'].isin(book_ids)]\n",
    "books_clean_ids = books_clean['Book-ID'].unique()\n",
    "percent_books_missing = (num_books-len(books_clean_ids))/num_books*100\n",
    "\n",
    "print(f'There are {round(percent_books_missing, 2)}% of books in the graph missing in the books data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retaining users who have at least one book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users['New-User-ID'] = users['User-ID'].map(lessen_user_ids)\n",
    "users_clean = users[users['New-User-ID'].isin(user_ids)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fill in missing value Age with simple imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_imputer = KNNImputer(n_neighbors=5)\n",
    "users_clean['Age'] = knn_imputer.fit_transform(users_clean[['Age']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1/5 of the books that have rating information do not have further information on the books dataset. However, as our objective is to investigate an user-based recommender system, this is irrelevant. We are able to embed the age and location data of users. As the age data is sparse, location data will be our main source of information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make graph\n",
    "src = torch.tensor(ratings['New-User-ID'].values)\n",
    "dst = torch.tensor(ratings['Book-ID'].values)\n",
    "print(len(src), len(dst))\n",
    "\n",
    "edges = {\n",
    "    ('user', 'rating', 'book'): (src, dst)\n",
    "}\n",
    "\n",
    "g = dgl.heterograph(edges, num_nodes_dict={'user': num_users, 'book': num_books})\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weigth the edges by ratings\n",
    "rating_data = ratings['Book-Rating'].values\n",
    "ratings_tensor = torch.tensor(rating_data, dtype=torch.float32)\n",
    "g.edges['rating'].data['rating'] = ratings_tensor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Add age to user feature\n",
    "# ages = users_clean['Age'].values\n",
    "# ages_tensor = torch.tensor(ages, dtype=torch.float32)\n",
    "# g.nodes['user'].data['age'] = ages_tensor\n",
    "\n",
    "# Add age to user feature\n",
    "ages = users_clean.set_index('New-User-ID')['Age'].sort_index().values\n",
    "ages_tensor = torch.tensor(ages, dtype=torch.float32)\n",
    "g.nodes['user'].data['age'] = ages_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extract the country from the location by obtaining the expression after the last comma in e.g. nyc, new york, usa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_clean['Country'] = users_clean['Location'].str.rsplit(',', n=1).str[-1].str.strip()\n",
    "country_counts = users_clean['Country'].value_counts(normalize=True)\n",
    "\n",
    "# We see that less frequent locations do not always contain country names, so we remove values of locations representing less than 1%\n",
    "rare_countries = country_counts[country_counts < 0.01].index\n",
    "users_clean.loc[users_clean['Country'].isin(rare_countries), 'Country'] = np.nan\n",
    "\n",
    "# Encoding Countries to a unique interger (same as label encoding)\n",
    "country_ids = {country: idx for idx, country in enumerate(users_clean['Country'].unique())}  # map country to a unique integer\n",
    "users_clean['CountryId'] = users_clean['Country'].map(country_ids)\n",
    "\n",
    "country_ids = {country: idx for idx, country in enumerate(users_clean['Country'].dropna().unique())}\n",
    "users_clean['CountryId'] = users_clean['Country'].map(country_ids).fillna(-1).astype(int)\n",
    "\n",
    "countries = users_clean['CountryId'].values\n",
    "\n",
    "# countries = users_clean.set_index('New-User-ID')['CountryId'].sort_index().fillna(-1).values  # Fill NaN with -1\n",
    "\n",
    "countries_tensor = torch.tensor(countries, dtype=torch.float32)\n",
    "g.nodes['user'].data['country'] = countries_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(g.nodes['user'])\n",
    "print(g.nodes['book'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic graph info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(g)  # Prints the basic info of the graph, such as number of nodes and edges per type\n",
    "\n",
    "# Print number of nodes for each type\n",
    "print(\"Number of users:\", g.number_of_nodes('user'))\n",
    "print(\"Number of books:\", g.number_of_nodes('book'))\n",
    "\n",
    "# Print number of edges\n",
    "print(\"Number of ratings:\", g.number_of_edges('rating'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node and Edge feature inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print user node features\n",
    "print(\"User features:\", g.nodes['user'].data.keys())\n",
    "\n",
    "# Print book node features, if any\n",
    "print(\"Book features:\", g.nodes['book'].data.keys())\n",
    "\n",
    "# Print edge features\n",
    "print(\"Edge features:\", g.edges['rating'].data.keys())\n",
    "\n",
    "# Example to print specific feature details:\n",
    "print(\"Sample user ages:\", g.nodes['user'].data['age'][:5])  # prints first 5 user ages\n",
    "print(\"Sample ratings:\", g.edges['rating'].data['rating'][:5])  # prints first 5 ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eliminate isolated nodes if any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compact_g = dgl.compact_graphs(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create synthetic features for book based on degree of the node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_in_degrees = compact_g.in_degrees(etype=('user', 'rating', 'book')).float().unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compact_g.nodes['book'].data['in_degree'] = book_in_degrees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the GNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class GNNRecommender(nn.Module):\n",
    "#     def __init__(self, user_feats, book_feats, hidden_size, num_classes):\n",
    "#         super(GNNRecommender, self).__init__()\n",
    "#         self.user_conv = GraphConv(user_feats, hidden_size, allow_zero_in_degree=True)\n",
    "#         self.book_conv = GraphConv(book_feats, hidden_size, allow_zero_in_degree=True)\n",
    "#         self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "#     def forward(self, g, user_features, book_features):\n",
    "#         user_h = F.relu(self.user_conv(g, user_features))\n",
    "#         book_h = F.relu(self.book_conv(g, book_features))\n",
    "#         user_book_h = user_h + book_h\n",
    "#         return self.fc(user_book_h)\n",
    "\n",
    "class GNNRecommender(nn.Module):\n",
    "    def __init__(self, user_feats_dim, book_feats_dim, hidden_size, num_classes):\n",
    "        super(GNNRecommender, self).__init__()\n",
    "        self.user_conv = GraphConv(user_feats_dim, hidden_size, allow_zero_in_degree=True)\n",
    "        self.book_conv = GraphConv(book_feats_dim, hidden_size, allow_zero_in_degree=True)\n",
    "        self.fc = nn.Linear(hidden_size * 2, num_classes)  # *2 for concatenation of user and book hidden states\n",
    "        \n",
    "        print(self.user_conv)\n",
    "        print(self.book_conv)\n",
    "        print(self.fc)\n",
    "        \n",
    "    def forward(self, g, user_features, book_features):\n",
    "        user_h = F.relu(self.user_conv(g, user_features))\n",
    "        book_h = F.relu(self.book_conv(g, book_features))\n",
    "        \n",
    "        # # Combine user and book features; here we concatenate\n",
    "        # combined_h = torch.cat([user_h, book_h], dim=1)\n",
    "        \n",
    "        # Combine user and book features (this is just one way; ensure it aligns with your task)\n",
    "        combined_h = torch.cat([user_h, book_h], dim=0)\n",
    "        return self.fc(combined_h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGED g. to compact_g.\n",
    "\n",
    "# Create USER Features Tensor\n",
    "age_tensor = compact_g.nodes['user'].data['age'].unsqueeze(1)\n",
    "country_tensor = compact_g.nodes['user'].data['country'].unsqueeze(1)\n",
    "user_feats = torch.cat([age_tensor, country_tensor], dim=1)\n",
    "\n",
    "# Create BOOK Features Tensor\n",
    "book_feats = compact_g.nodes['book'].data['in_degree']\n",
    "\n",
    "user_feat_dim = user_feats.shape[1]  # the size of user feature\n",
    "book_feat_dim = book_feats.shape[1]  # the size of book feature\n",
    "\n",
    "print(f\"User feature dimension {user_feat_dim}\")\n",
    "print(f\"Book feature dimension {book_feat_dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 32\n",
    "num_classes = 1  # predicting a single rating value\n",
    "model = GNNRecommender(user_feat_dim, book_feat_dim, hidden_size, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split graph for training and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_graph(g, proportion=0.8):\n",
    "    # Split edges randomly for training and validation\n",
    "    num_edges = g.number_of_edges('rating')\n",
    "    all_edges = np.arange(num_edges)\n",
    "    np.random.shuffle(all_edges)\n",
    "    \n",
    "    train_size = int(num_edges * proportion)\n",
    "    train_edges = all_edges[:train_size]\n",
    "    val_edges = all_edges[train_size:]\n",
    "    \n",
    "    # # Create subgraphs based on the edges (change to True)\n",
    "    g_train = dgl.edge_subgraph(g, train_edges, relabel_nodes=True)\n",
    "    g_val = dgl.edge_subgraph(g, val_edges, relabel_nodes=True)\n",
    "    \n",
    "    # Create subgraphs based on the edges\n",
    "    # g_train = dgl.edge_subgraph(g, {'rating': train_edges}, relabel_nodes=False)\n",
    "    # g_val = dgl.edge_subgraph(g, {'rating': val_edges}, relabel_nodes=False)\n",
    "    \n",
    "    return g_train, g_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_train, g_val = split_graph(compact_g, proportion=0.8)\n",
    "\n",
    "# Verify subgraphs\n",
    "print(\"Number of users in training graph:\", g_train.number_of_nodes('user'))\n",
    "print(\"Number of books in training graph:\", g_train.number_of_nodes('book'))\n",
    "print(\"Number of ratings in training graph:\", g_train.number_of_edges('rating'), \"\\n\")\n",
    "\n",
    "print(\"Number of users in validation graph:\", g_val.number_of_nodes('user'))\n",
    "print(\"Number of books in validation graph:\", g_val.number_of_nodes('book'))\n",
    "print(\"Number of ratings in validation graph:\", g_val.number_of_edges('rating'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the structure of the edge data\n",
    "print(g_train.edges['rating'].data)\n",
    "print(g_val.edges['rating'].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the features and ratings for the Training Set\n",
    "age_tensor_train = g_train.nodes['user'].data['age'].unsqueeze(1) # (N, 1)\n",
    "country_tensor_train = g_train.nodes['user'].data['country'].unsqueeze(1) # (N, 1)\n",
    "\n",
    "assert age_tensor_train.shape[0] == country_tensor_train.shape[0], \"Mismatch in number of users\"\n",
    "\n",
    "\n",
    "# Training Set\n",
    "user_features_train = torch.cat([age_tensor_train, country_tensor_train], dim=1) # (N, 2)\n",
    "book_features_train = g_train.nodes['book'].data['in_degree'] # (M, 1)\n",
    "\n",
    "ratings_train = g_train.edges['rating'].data['rating']\n",
    "\n",
    "# Add user and book features\n",
    "g_train.nodes['user'].data['features'] = user_features_train\n",
    "g_train.nodes['book'].data['features'] = book_features_train\n",
    "\n",
    "\n",
    "# Get the features and ratings for the Validation Set\n",
    "age_tensor_val = g_val.nodes['user'].data['age'].unsqueeze(1) # (N_val, 1)\n",
    "country_tensor_val = g_val.nodes['user'].data['country'].unsqueeze(1) # (N_val, 1)\n",
    "\n",
    "assert age_tensor_val.shape[0] == country_tensor_val.shape[0], \"Mismatch in number of validation users\"\n",
    "\n",
    "# Validation Set\n",
    "user_features_val = torch.cat([age_tensor_val, country_tensor_val], dim=1) # (N_val, 2)\n",
    "book_features_val = g_val.nodes['book'].data['in_degree'] # (M_val, 1)\n",
    "ratings_val = g_val.edges['rating'].data['rating']\n",
    "\n",
    "# Verify feature dimensions\n",
    "print(\"User features train shape:\", user_features_train.shape)\n",
    "print(\"Book features train shape:\", book_features_train.shape)\n",
    "print(\"Ratings train shape:\", ratings_train.shape, \"\\n\")\n",
    "\n",
    "print(\"User features val shape:\", user_features_val.shape)\n",
    "print(\"Book features val shape:\", book_features_val.shape)\n",
    "print(\"Ratings val shape:\", ratings_val.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_train.edges['rating'].data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, g, user_features, book_features, labels, optimizer, criterion):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(g, user_features, book_features)\n",
    "    # Added .squeeze()\n",
    "    loss = criterion(outputs.squeeze(), labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 2  # or however many epochs you deem necessary\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    loss = train(model, g_train, user_features_train, book_features_train, ratings_train, optimizer, criterion)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, g, user_features, book_features, labels, criterion):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(g, user_features, book_features)\n",
    "        # Added .squeeze()\n",
    "        loss = criterion(outputs.squeeze(), labels)\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation loss\n",
    "validation_loss = evaluate(model, g_val, user_features_val, book_features_val, ratings_val, criterion)\n",
    "print(f'Validation Loss: {validation_loss}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.5 ('env_GNN': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "8f6377febe87314814c7b7161cea679ad38a9298f17e15a391a898e937fe96e0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
