{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Book Recommender System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For Pre-Processing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# For GNN\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import networkx as nx\n",
    "import dgl\n",
    "import dgl.function as fn\n",
    "from dgl.nn import GraphConv\n",
    "from dgl.nn import HeteroGNNExplainer\n",
    "import dgl.nn.pytorch as dglnn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0f/xqjk7pr56mjd12xvy1w435_40000gn/T/ipykernel_1874/3661012832.py:2: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  books = pd.read_csv(\"Data/Books.csv\")\n"
     ]
    }
   ],
   "source": [
    "ratings = pd.read_csv(\"Data/Ratings.csv\")\n",
    "books = pd.read_csv(\"Data/Books.csv\")\n",
    "users = pd.read_csv(\"Data/Users.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "- We will only use users and books present in the ratings dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_country(location):\n",
    "    if not location:\n",
    "        return None\n",
    "    parts = [part.strip() for part in location.split(',')]\n",
    "    \n",
    "    return parts[-1] if parts and parts[-1] else None\n",
    "\n",
    "users['Country'] = users['Location'].apply(extract_country)\n",
    "\n",
    "# label_encoder = LabelEncoder()\n",
    "# users['Country'] = label_encoder.fit_transform(users['Country'])\n",
    "\n",
    "# Removing Book Ratings that have a 0 rating\n",
    "ratings = ratings[ratings['Book-Rating'] > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering for Books and Users that have a rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of User IDs in raitngs: 77805\n",
      "There are: 185973, unique book IDs\n",
      "There are 77805 unique users, and 185973 unique books in the ratings dataset.\n",
      "\n",
      "There are: 149836, books that have an ISBN\n",
      "There are around 19.0% of books in the graph missing in the books data\n",
      "There are: 77805, who have rated at least one book\n"
     ]
    }
   ],
   "source": [
    "# Renaming User IDs\n",
    "rename_user_ids = {userid: idx for idx, userid in enumerate(ratings['User-ID'].unique())}\n",
    "# Mapping new User IDs to Users that have a rating\n",
    "ratings['New-User-ID'] = ratings['User-ID'].map(rename_user_ids)\n",
    "# Getting the unique User IDs Ratings\n",
    "ratings_user_ids = list(ratings['New-User-ID'].unique())\n",
    "print(f\"Number of User IDs in raitngs: {len(ratings_user_ids)}\")\n",
    "\n",
    "# ISBN in Ratings Data sets\n",
    "isbn_to_id = {isbn: idx for idx, isbn in enumerate(ratings['ISBN'].unique())}\n",
    "# Map new ISBN to Books\n",
    "ratings['New-Book-ISBN'] = ratings['ISBN'].map(isbn_to_id)\n",
    "# Get unique Book Ratings \n",
    "ratings_book_ids = list(ratings['New-Book-ISBN'].unique())\n",
    "print(f\"There are: {len(ratings_book_ids)}, unique book IDs\")\n",
    "\n",
    "print(f'There are {len(ratings_user_ids)} unique users, and {len(ratings_book_ids)} unique books in the ratings dataset.\\n')\n",
    "\n",
    "# ===========================================================================================================================\n",
    "# ===========================================================================================================================\n",
    "\n",
    "# ISBN in Books Data set\n",
    "books['New-Book-ISBN'] = books['ISBN'].map(isbn_to_id)\n",
    "# Filtering for books that have a rating\n",
    "books_clean = books[books['New-Book-ISBN'].isin(ratings_book_ids)]\n",
    "\n",
    "print(f\"There are: {len(books_clean['New-Book-ISBN'].unique())}, books that have an ISBN\")\n",
    "\n",
    "books_clean_ids = books_clean['New-Book-ISBN'].unique()\n",
    "percent_books_missing = round((len(ratings_book_ids)-len(books_clean_ids))/len(ratings_book_ids)*100, 0)\n",
    "\n",
    "print(f'There are around {percent_books_missing}% of books in the graph missing in the books data')\n",
    "\n",
    "users['New-User-ID'] = users['User-ID'].map(rename_user_ids)\n",
    "users_clean = users[users['New-User-ID'].isin(ratings_user_ids)]\n",
    "print(f\"There are: {len(users_clean['New-User-ID'])}, who have rated at least one book\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aroung 1/5 of the books that have rating information do not have further information on the books dataset. However, as our objective is to investigate an user-based recommender system, this is irrelevant. We are able to embed the age and location data of users. As the age data is sparse, location data will be our main source of information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User-ID</th>\n",
       "      <th>ISBN</th>\n",
       "      <th>Book-Rating</th>\n",
       "      <th>New-User-ID</th>\n",
       "      <th>New-Book-ISBN</th>\n",
       "      <th>AVG_Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>276726</td>\n",
       "      <td>0155061224</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>276729</td>\n",
       "      <td>052165615X</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>276729</td>\n",
       "      <td>0521795028</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>276736</td>\n",
       "      <td>3257224281</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>6.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>276737</td>\n",
       "      <td>0600570967</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>6.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   User-ID        ISBN  Book-Rating  New-User-ID  New-Book-ISBN  AVG_Rating\n",
       "0   276726  0155061224            5            0              0        5.00\n",
       "1   276729  052165615X            3            1              1        3.00\n",
       "2   276729  0521795028            6            1              2        6.00\n",
       "3   276736  3257224281            8            2              3        6.75\n",
       "4   276737  0600570967            6            3              4        6.00"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Including average rating\n",
    "avg_rating = ratings.groupby(\"New-Book-ISBN\")[\"Book-Rating\"].mean().reset_index()\n",
    "avg_rating = avg_rating.rename(columns={'Book-Rating': 'AVG_Rating'})\n",
    "\n",
    "ratings = ratings.merge(avg_rating, on=\"New-Book-ISBN\", how='left')\n",
    "\n",
    "ratings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fill in missing value Age with simple imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0f/xqjk7pr56mjd12xvy1w435_40000gn/T/ipykernel_1874/4026430534.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  users_clean['Age'] = knn_imputer.fit_transform(users_clean[['Age']])\n"
     ]
    }
   ],
   "source": [
    "knn_imputer = KNNImputer(n_neighbors=5)\n",
    "users_clean['Age'] = knn_imputer.fit_transform(users_clean[['Age']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "433671 433671\n",
      "There are 77805 users, and 185973 books\n",
      "Graph(num_nodes={'book': 185973, 'user': 77805},\n",
      "      num_edges={('user', 'rating', 'book'): 433671},\n",
      "      metagraph=[('user', 'book', 'rating')])\n"
     ]
    }
   ],
   "source": [
    "src_tensor = torch.tensor(ratings['New-User-ID'].values)\n",
    "dst_tensor = torch.tensor(ratings['New-Book-ISBN'].values)\n",
    "\n",
    "# Users and Books datasets are metadata\n",
    "\n",
    "print(len(src_tensor), len(dst_tensor))\n",
    "\n",
    "num_users = len(ratings_user_ids)\n",
    "num_books = len(ratings_book_ids)\n",
    "print(f\"There are {num_users} users, and {num_books} books\")\n",
    "\n",
    "# # Initialize the adjacency matrix with zeros\n",
    "# adjacency_matrix = np.zeros((num_users, num_books))\n",
    "# # Populate the adjacency matrix\n",
    "# for user, book, rating in zip(src_tensor, dst_tensor, ratings_values):\n",
    "#     adjacency_matrix[user, book] = rating\n",
    "# print(f\"Size of the adjacency matrix: {adjacency_matrix.shape}\")\n",
    "\n",
    "# Dictionary which defines the Heterograph\n",
    "edges = {\n",
    "    ('user', 'rating', 'book'): (src_tensor, dst_tensor)\n",
    "}\n",
    "g = dgl.heterograph(edges, num_nodes_dict={'user': num_users, 'book': num_books})\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weigth the edges by ratings\n",
    "rating_data = ratings['Book-Rating'].values\n",
    "g.edges['rating'].data['rating'] = torch.tensor(rating_data, dtype=torch.float32) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add age to user feature\n",
    "ages = users_clean.set_index('New-User-ID')['Age'].sort_index().values\n",
    "g.nodes['user'].data['age'] =  torch.tensor(ages, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7            canada\n",
       "8               usa\n",
       "9             spain\n",
       "11              usa\n",
       "13              usa\n",
       "            ...    \n",
       "278845       canada\n",
       "278848       canada\n",
       "278850          usa\n",
       "278851    australia\n",
       "278853          usa\n",
       "Name: Country, Length: 77805, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_clean[\"Country\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extract the country from the location by obtaining the expression after the last comma in e.g. nyc, new york, usa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0f/xqjk7pr56mjd12xvy1w435_40000gn/T/ipykernel_1874/1079567941.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  users_clean['Country'] = users_clean['Location'].str.rsplit(',', n=1).str[-1].str.strip()\n"
     ]
    }
   ],
   "source": [
    "# Extracting only the Country from the Location\n",
    "users_clean['Country'] = users_clean['Location'].str.rsplit(',', n=1).str[-1].str.strip()\n",
    "# Country Frequency\n",
    "country_counts = users_clean['Country'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that less frequent locations do not always contain country names, so we remove values of locations representing less than 1%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rare_countries = country_counts[country_counts < 0.01].index\n",
    "users_clean.loc[users_clean['Country'].isin(rare_countries), 'Country'] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding Countries to a unique interger (same as label encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0f/xqjk7pr56mjd12xvy1w435_40000gn/T/ipykernel_1874/3859667754.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  users_clean['CountryId'] = users_clean['Country'].map(country_ids).fillna(-1).astype(int)\n"
     ]
    }
   ],
   "source": [
    "# country_ids = {country: idx for idx, country in enumerate(users_clean['Country'].unique())}  # map country to a unique integer\n",
    "# users_clean['CountryId'] = users_clean['Country'].map(country_ids)\n",
    "\n",
    "country_ids = {country: idx for idx, country in enumerate(users_clean['Country'].dropna().unique())}\n",
    "users_clean['CountryId'] = users_clean['Country'].map(country_ids).fillna(-1).astype(int)\n",
    "\n",
    "countries_value_count = users_clean['CountryId'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountryId\n",
      " 1    45368\n",
      " 0     6986\n",
      "-1     6273\n",
      " 4     4445\n",
      " 6     4040\n",
      " 2     2643\n",
      " 8     2499\n",
      " 3     2386\n",
      " 7     2191\n",
      " 5      974\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(countries_value_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = users_clean['CountryId'].values\n",
    "countries_tensor = torch.tensor(countries, dtype=torch.float32)\n",
    "g.nodes['user'].data['country'] = countries_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding Average Ratings as Meta Data to books nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_book_ratings = ratings.drop_duplicates(subset='New-Book-ISBN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding Average Books\n",
    "book_avg_rating = unique_book_ratings[\"AVG_Rating\"]\n",
    "avg_rating_tensor = torch.tensor(book_avg_rating, dtype=torch.float32)\n",
    "g.nodes['book'].data['AVG_Rating'] = avg_rating_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic graph info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NodeSpace(data={'age': tensor([35.8179, 16.0000, 35.8179,  ..., 38.0000, 14.0000, 12.0000]), 'country': tensor([0., 1., 2.,  ..., 1., 8., 1.])})\n",
      "NodeSpace(data={'AVG_Rating': tensor([5., 3., 6.,  ..., 5., 5., 8.])})\n"
     ]
    }
   ],
   "source": [
    "print(g.nodes['user'])\n",
    "print(g.nodes['book'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph(num_nodes={'book': 185973, 'user': 77805},\n",
      "      num_edges={('user', 'rating', 'book'): 433671},\n",
      "      metagraph=[('user', 'book', 'rating')])\n",
      "Number of users: 77805\n",
      "Number of books: 185973\n",
      "Number of ratings: 433671\n"
     ]
    }
   ],
   "source": [
    "print(g)  # Prints the basic info of the graph, such as number of nodes and edges per type\n",
    "\n",
    "# Print number of nodes for each type\n",
    "print(\"Number of users:\", g.number_of_nodes('user'))\n",
    "print(\"Number of books:\", g.number_of_nodes('book'))\n",
    "\n",
    "# Print number of edges\n",
    "print(\"Number of ratings:\", g.number_of_edges('rating'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node and Edge feature inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User features: dict_keys(['age', 'country'])\n",
      "Book features: dict_keys(['AVG_Rating'])\n",
      "Edge features: dict_keys(['rating'])\n",
      "Sample user ages: tensor([35.8179, 16.0000, 35.8179, 14.0000, 35.8179])\n",
      "Sample ratings: tensor([5., 3., 6., 8., 6.])\n"
     ]
    }
   ],
   "source": [
    "# Print user node features\n",
    "print(\"User features:\", g.nodes['user'].data.keys())\n",
    "\n",
    "# Print book node features, if any\n",
    "print(\"Book features:\", g.nodes['book'].data.keys())\n",
    "\n",
    "# Print edge features\n",
    "print(\"Edge features:\", g.edges['rating'].data.keys())\n",
    "\n",
    "# Example to print specific feature details:\n",
    "print(\"Sample user ages:\", g.nodes['user'].data['age'][:5])  # prints first 5 user ages\n",
    "print(\"Sample ratings:\", g.edges['rating'].data['rating'][:5])  # prints first 5 ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eliminate isolated nodes if any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "compact_g = dgl.compact_graphs(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create synthetic features for book based on degree of the node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_in_degrees = compact_g.in_degrees(etype=('user', 'rating', 'book')).float().unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "compact_g.nodes['book'].data['in_degree'] = book_in_degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = compact_g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define node features (for simplicity, we use random features here, replace with actual features if available)\n",
    "user_features = torch.randn(num_users, 10)  # 10-dimensional features for users\n",
    "book_features = torch.randn(num_books, 10)  # 10-dimensional features for books\n",
    "\n",
    "# Combine features into a single tensor, while keeping track of user and book indices\n",
    "features = torch.cat([user_features, book_features], dim=0)\n",
    "\n",
    "# Create and train the model\n",
    "model = LinkPredictor(in_feats=10, hidden_feats=16, out_feats=8)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    scores = model(g, features)\n",
    "    \n",
    "    # Assuming we have ground truth scores\n",
    "    ground_truth = ratings_values\n",
    "    \n",
    "    loss = loss_fn(scores, ground_truth)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f'Epoch {epoch}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine user and book features into tensors\n",
    "user_features = torch.cat([g.nodes['user'].data['age'].unsqueeze(1), g.nodes['user'].data['country'].unsqueeze(1)], dim=1)\n",
    "book_features = torch.cat([g.nodes['book'].data['AVG_Rating'].unsqueeze(1), g.nodes['book'].data['in_degree']], dim=1)\n",
    "\n",
    "# Edge features (ratings)\n",
    "edge_weights = g.edges['rating'].data['rating']\n",
    "\n",
    "# Define input feature dimensions and number of classes\n",
    "user_input_dim = user_features.shape[1]\n",
    "book_input_dim = book_features.shape[1]\n",
    "h_feats = 16  # Number of hidden features\n",
    "out_feats = 1  # Regression task (predicting ratings)\n",
    "\n",
    "# Initialize the model\n",
    "model = HeteroGNN(user_input_dim, book_input_dim, h_feats, out_feats)\n",
    "\n",
    "# Print model architecture\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training settings\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_fn = nn.MSELoss()  # Mean Squared Error for regression tasks\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 2\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    user_logits, book_logits = model(g, user_features, book_features, edge_weights)  # Forward pass\n",
    "\n",
    "    # Combine user and book embeddings to predict ratings\n",
    "    user_emb = user_logits[g.edges(etype='rating')[0]]\n",
    "    book_emb = book_logits[g.edges(etype='rating')[1]]\n",
    "    predicted_ratings = (user_emb + book_emb).mean(dim=1)  # Simplistic approach for combining embeddings\n",
    "    loss = loss_fn(predicted_ratings, g.edges['rating'].data['rating'])\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}/{num_epochs}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGED g. to compact_g.\n",
    "\n",
    "# Create USER Features Tensor\n",
    "age_tensor = compact_g.nodes['user'].data['age'].unsqueeze(1)\n",
    "country_tensor = compact_g.nodes['user'].data['country'].unsqueeze(1)\n",
    "user_feats = torch.cat([age_tensor, country_tensor], dim=1)\n",
    "\n",
    "# Create BOOK Features Tensor\n",
    "book_feats = compact_g.nodes['book'].data['in_degree']\n",
    "\n",
    "user_feat_dim = user_feats.shape[1]  # the size of user feature\n",
    "book_feat_dim = book_feats.shape[1]  # the size of book feature\n",
    "\n",
    "print(f\"User feature dimension {user_feat_dim}\")\n",
    "print(f\"Book feature dimension {book_feat_dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 32\n",
    "num_classes = 1  # predicting a single rating value\n",
    "model = GNNRecommender(user_feat_dim, book_feat_dim, hidden_size, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split graph for training and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_graph(g, proportion=0.8):\n",
    "    # Split edges randomly for training and validation\n",
    "    num_edges = g.number_of_edges('rating')\n",
    "    all_edges = np.arange(num_edges)\n",
    "    np.random.shuffle(all_edges)\n",
    "    \n",
    "    train_size = int(num_edges * proportion)\n",
    "    train_edges = all_edges[:train_size]\n",
    "    val_edges = all_edges[train_size:]\n",
    "    \n",
    "    # # Create subgraphs based on the edges (change to True)\n",
    "    g_train = dgl.edge_subgraph(g, train_edges, relabel_nodes=True)\n",
    "    g_val = dgl.edge_subgraph(g, val_edges, relabel_nodes=True)\n",
    "    \n",
    "    # Create subgraphs based on the edges\n",
    "    # g_train = dgl.edge_subgraph(g, {'rating': train_edges}, relabel_nodes=False)\n",
    "    # g_val = dgl.edge_subgraph(g, {'rating': val_edges}, relabel_nodes=False)\n",
    "    \n",
    "    return g_train, g_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_train, g_val = split_graph(compact_g, proportion=0.8)\n",
    "\n",
    "# Verify subgraphs\n",
    "print(\"Number of users in training graph:\", g_train.number_of_nodes('user'))\n",
    "print(\"Number of books in training graph:\", g_train.number_of_nodes('book'))\n",
    "print(\"Number of ratings in training graph:\", g_train.number_of_edges('rating'), \"\\n\")\n",
    "\n",
    "print(\"Number of users in validation graph:\", g_val.number_of_nodes('user'))\n",
    "print(\"Number of books in validation graph:\", g_val.number_of_nodes('book'))\n",
    "print(\"Number of ratings in validation graph:\", g_val.number_of_edges('rating'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the structure of the edge data\n",
    "print(g_train.edges['rating'].data)\n",
    "print(g_val.edges['rating'].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the features and ratings for the Training Set\n",
    "age_tensor_train = g_train.nodes['user'].data['age'].unsqueeze(1) # (N, 1)\n",
    "country_tensor_train = g_train.nodes['user'].data['country'].unsqueeze(1) # (N, 1)\n",
    "\n",
    "assert age_tensor_train.shape[0] == country_tensor_train.shape[0], \"Mismatch in number of users\"\n",
    "\n",
    "\n",
    "# Training Set\n",
    "user_features_train = torch.cat([age_tensor_train, country_tensor_train], dim=1) # (N, 2)\n",
    "book_features_train = g_train.nodes['book'].data['in_degree'] # (M, 1)\n",
    "\n",
    "ratings_train = g_train.edges['rating'].data['rating']\n",
    "\n",
    "# Add user and book features\n",
    "g_train.nodes['user'].data['features'] = user_features_train\n",
    "g_train.nodes['book'].data['features'] = book_features_train\n",
    "\n",
    "\n",
    "# Get the features and ratings for the Validation Set\n",
    "age_tensor_val = g_val.nodes['user'].data['age'].unsqueeze(1) # (N_val, 1)\n",
    "country_tensor_val = g_val.nodes['user'].data['country'].unsqueeze(1) # (N_val, 1)\n",
    "\n",
    "assert age_tensor_val.shape[0] == country_tensor_val.shape[0], \"Mismatch in number of validation users\"\n",
    "\n",
    "# Validation Set\n",
    "user_features_val = torch.cat([age_tensor_val, country_tensor_val], dim=1) # (N_val, 2)\n",
    "book_features_val = g_val.nodes['book'].data['in_degree'] # (M_val, 1)\n",
    "ratings_val = g_val.edges['rating'].data['rating']\n",
    "\n",
    "# Verify feature dimensions\n",
    "print(\"User features train shape:\", user_features_train.shape)\n",
    "print(\"Book features train shape:\", book_features_train.shape)\n",
    "print(\"Ratings train shape:\", ratings_train.shape, \"\\n\")\n",
    "\n",
    "print(\"User features val shape:\", user_features_val.shape)\n",
    "print(\"Book features val shape:\", book_features_val.shape)\n",
    "print(\"Ratings val shape:\", ratings_val.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_train.edges['rating'].data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, g, user_features, book_features, labels, optimizer, criterion):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(g, user_features, book_features)\n",
    "    # Added .squeeze()\n",
    "    loss = criterion(outputs.squeeze(), labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 2  # or however many epochs you deem necessary\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    loss = train(model, g_train, user_features_train, book_features_train, ratings_train, optimizer, criterion)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, g, user_features, book_features, labels, criterion):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(g, user_features, book_features)\n",
    "        # Added .squeeze()\n",
    "        loss = criterion(outputs.squeeze(), labels)\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation loss\n",
    "validation_loss = evaluate(model, g_val, user_features_val, book_features_val, ratings_val, criterion)\n",
    "print(f'Validation Loss: {validation_loss}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.5 ('env_GNN': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "8f6377febe87314814c7b7161cea679ad38a9298f17e15a391a898e937fe96e0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
